version: "0.2.0"
system: "You are a helpful, concise assistant."
gen:
  model: "llama3.1:8b"
  temperature: 0.4
  num_predict: 200
memory:
  embed_model: "nomic-embed-text"
  top_k: 4
  # memory backend: chroma | jsonl
  backend: jsonl
  # if backend=jsonl
  jsonl_path: memory.jsonl
  # if backend=chroma
  chroma_path: data/chroma
  collection: memory
  deduplication:
    enabled: true
    threshold: 0.99
safety:
  banned_keywords:
    - "make a bomb"
  warn_on_long_answer: 1200
disclaimer: "For research use only; responses may be inaccurate."
# New: token budget defaults (tune in Week 3)
budget:
  context_capacity: 8192   # C
  buffer: 128              # ε
  max_retrieved_chunks: 5  # soft cap for R
logging:
  dir: "runs"
# add this block
external_memory:
  enabled: true
  top_k: 4
  embed_model: "nomic-embed-text"   # embedding model for external sources
  # Additional Chroma collections to query as knowledge stores
  chroma:
    - path: "data/chroma"
      collection: "wiki"
    - path: "data/chroma"
      collection: "longmem_docs"
  # Multi-source configuration for future MultiChromaRAG integration
  chroma_sources:
    - name: "wiki"
      db_path: "data/chroma"
      collection: "wiki_mini"
      embed_model: "all-minilm"
      top_k: 4
    - name: "longmem"
      db_path: "data/chroma"
      collection: "longmem_docs"
      embed_model: "all-minilm"
      top_k: 4

router:
  enabled: true
  bundle_path: "artifacts/memgpt_router.joblib"   # created by train_router.py

# We’ll enforce S + U + M + R + G + ε ≤ C in Week 3. For Week 1, we only count tokens.



